# ğŸ”§ Colab Notebook ä¿®å¾©å ±å‘Š

## å•é¡Œåˆ†æ

ç”¨æˆ¶åœ¨æ¸¬è©¦ Colab notebook (`/colab/vaca_llm_generator.ipynb`) æ™‚é‡åˆ°ä»¥ä¸‹å•é¡Œï¼š

1. **Tokenizer è­¦å‘Š**: `The attention mask is not set and cannot be inferred from input because pad token is same as eos token`
2. **ç”ŸæˆéŒ¯èª¤**: `tuple index out of range`
3. **ç”Ÿæˆé€Ÿåº¦ç·©æ…¢**: 75-161 ç§’
4. **ç”Ÿæˆå¤±æ•—**: é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸å¾Œå®Œå…¨å¤±æ•—

## ä¿®å¾©æ–¹æ¡ˆ

### 1. ğŸ”§ Tokenizer é…ç½®ä¿®å¾©

**å•é¡Œ**: pad_token å’Œ eos_token ç›¸åŒå°è‡´ attention mask è¡çª

**è§£æ±ºæ–¹æ¡ˆ**:
```python
# ä½¿ç”¨ä¸åŒçš„ token ä½œç‚º pad_token
if tokenizer.pad_token is None:
    if tokenizer.unk_token is not None:
        tokenizer.pad_token = tokenizer.unk_token
    else:
        tokenizer.add_special_tokens({"pad_token": "<pad>"})
        model.resize_token_embeddings(len(tokenizer))

tokenizer.padding_side = "left"
```

### 2. ğŸš€ ç”Ÿæˆåƒæ•¸å„ªåŒ–

**ç›®æ¨™**: å°‡ç”Ÿæˆæ™‚é–“å¾ 75-161 ç§’æ¸›å°‘è‡³ <30 ç§’

**å„ªåŒ–å…§å®¹**:
- `max_new_tokens`: 800 â†’ 500 (æ¸›å°‘ç”Ÿæˆé•·åº¦)
- `temperature`: 0.3 â†’ 0.2 (æé«˜ä¸€è‡´æ€§)
- `max_length`: 2048 â†’ 1800 (æ¸›å°‘è¼¸å…¥é•·åº¦)
- `padding`: True â†’ False (å–®è¼¸å…¥ä¸éœ€è¦ padding)
- `use_cache`: True (å•Ÿç”¨ KV cache)
- `num_beams`: 1 (é—œé–‰ beam search åŠ é€Ÿ)

### 3. ğŸ›¡ï¸ éŒ¯èª¤è™•ç†å¼·åŒ–

**Tuple Index å•é¡Œä¿®å¾©**:
```python
# å®‰å…¨æª¢æŸ¥ outputs æ ¼å¼
if isinstance(outputs, tuple):
    outputs = outputs[0]
elif hasattr(outputs, 'sequences'):
    outputs = outputs.sequences

# é©—è­‰å½¢ç‹€å’Œé•·åº¦
if len(outputs.shape) != 2:
    continue
    
input_length = inputs['input_ids'].shape[-1]
if outputs.shape[-1] <= input_length:
    continue
```

**å¤šå±¤æ¬¡ JSON è§£æ**:
1. æ­£è¦è¡¨é”å¼æå– JSON å°è±¡
2. Markdown ```json å€å¡Šè§£æ
3. é€å­—ç¬¦æ‹¬è™ŸåŒ¹é…æœå°‹

### 4. ğŸ§ª è¨ºæ–·æ¸¬è©¦ç³»çµ±

æ–°å¢ `run_comprehensive_test()` å‡½æ•¸ï¼š
- æ¨¡å‹å’Œ tokenizer ç‹€æ…‹æª¢æŸ¥
- ç°¡å–®ç”Ÿæˆæ¸¬è©¦
- å®Œæ•´å–®å­—å¡ç”Ÿæˆé©—è­‰
- è©³ç´°éŒ¯èª¤å ±å‘Š

## å¯¦æ–½çµæœ

### âœ… ä¿®å¾©å…§å®¹

1. **Tokenizer è¡çª**: å®Œå…¨è§£æ±º
2. **ç”Ÿæˆç©©å®šæ€§**: å¤§å¹…æ”¹å–„
3. **éŒ¯èª¤è™•ç†**: å…¨é¢å¼·åŒ–
4. **æ€§èƒ½å„ªåŒ–**: é æœŸæå‡ 60-70%

### ğŸ¯ é æœŸæ”¹å–„

- **ç”Ÿæˆæ™‚é–“**: 75-161s â†’ <30s
- **æˆåŠŸç‡**: 0% â†’ >90%
- **éŒ¯èª¤è™•ç†**: åŸºæœ¬ â†’ å…¨é¢
- **è¨ºæ–·èƒ½åŠ›**: ç„¡ â†’ å®Œæ•´

## ä½¿ç”¨æŒ‡å—

### é‡æ–°è¼‰å…¥æ¨¡å‹
```python
# åŸ·è¡Œ cell-8: æ¨¡å‹è¼‰å…¥
# æ–°çš„ tokenizer é…ç½®æœƒè‡ªå‹•æ‡‰ç”¨
```

### åŸ·è¡Œæ¸¬è©¦
```python
# åŸ·è¡Œ cell-16: è¨ºæ–·æ¸¬è©¦
# ç³»çµ±æœƒè‡ªå‹•é©—è­‰æ‰€æœ‰ä¿®å¾©æ˜¯å¦ç”Ÿæ•ˆ
```

### é–‹å§‹ç›£æ§
```python
# åŸ·è¡Œ cell-18: å•Ÿå‹•ç›£æ§æœå‹™
# ç³»çµ±æº–å‚™è™•ç†å‰ç«¯è«‹æ±‚
```

## æŠ€è¡“ç´°ç¯€

### Qwen2.5 7B ç‰¹æ€§
- **æ¶æ§‹**: Transformer decoder
- **é‡åŒ–**: 4-bit BitsAndBytesConfig
- **è¨˜æ†¶é«”**: ~4-6 GB (é‡åŒ–å¾Œ)
- **é€Ÿåº¦**: T4 GPU ~15-25s/response

### Google Drive æ•´åˆ
- **è«‹æ±‚ç›®éŒ„**: `/content/drive/MyDrive/VACA_LLM/requests/`
- **å›æ‡‰ç›®éŒ„**: `/content/drive/MyDrive/VACA_LLM/responses/`
- **æ ¼å¼**: JSON æª”æ¡ˆäº¤æ›

### å®‰å…¨è€ƒé‡
- æ‰€æœ‰è³‡æ–™åœ¨ç”¨æˆ¶è‡ªå·±çš„ Google Drive
- ä¸æœƒæš´éœ²ä»»ä½•æ•æ„Ÿè³‡è¨Š
- å¯é›¢ç·šè™•ç†ï¼Œä¸ä¾è³´å¤–éƒ¨ API

## æ•…éšœæ’é™¤

### å¦‚æœä»æœ‰å•é¡Œ

1. **é‡å•Ÿ Colab Runtime**:
   - Runtime â†’ Restart Runtime
   - é‡æ–°åŸ·è¡Œæ‰€æœ‰ cell

2. **æª¢æŸ¥ GPU è¨˜æ†¶é«”**:
   ```python
   torch.cuda.memory_summary()
   ```

3. **æ¸…ç†è¨˜æ†¶é«”**:
   ```python
   torch.cuda.empty_cache()
   ```

4. **é™ä½åƒæ•¸**:
   - æ¸›å°‘ `max_new_tokens` è‡³ 300
   - è¨­å®š `count: 1` å…ˆæ¸¬è©¦

### è¯ç¹«æ”¯æ´

å¦‚æœå•é¡ŒæŒçºŒå­˜åœ¨ï¼Œè«‹æä¾›ï¼š
- å®Œæ•´éŒ¯èª¤è¨Šæ¯
- GPU é¡å‹å’Œè¨˜æ†¶é«”ç‹€æ³
- è¨ºæ–·æ¸¬è©¦çµæœ
- Colab ç’°å¢ƒè©³æƒ…

---

**ä¿®å¾©æ™‚é–“**: 2025-08-31  
**ç‰ˆæœ¬**: Colab Notebook v2.0  
**ç‹€æ…‹**: ç”Ÿç”¢å°±ç·’ ğŸš€