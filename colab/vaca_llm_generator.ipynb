{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yourusername/vaca-app/blob/main/colab/vaca_llm_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ VACA App - LLM å–®å­—å¡ç”Ÿæˆå™¨\n",
    "\n",
    "ä½¿ç”¨ Qwen2.5 7B æ¨¡å‹ç‚º VACA èƒŒå–®å­— App ç”Ÿæˆé«˜å“è³ªå–®å­—å¡ç‰‡ã€‚\n",
    "\n",
    "## åŠŸèƒ½ç‰¹è‰²ï¼š\n",
    "- ğŸ§  ä½¿ç”¨ Qwen2.5 7B Instruct æ¨¡å‹\n",
    "- ğŸ“š æ”¯æ´è€ƒè©¦æ¨™ç±¤ï¼ˆIELTSã€TOEFLã€GRE ç­‰ï¼‰\n",
    "- ğŸ¯ é¿å…å·²èƒŒå–®å­—é‡è¤‡\n",
    "- ğŸ“„ æ¨™æº– JSON æ ¼å¼è¼¸å‡º\n",
    "- ğŸ’¾ Google Drive æª”æ¡ˆäº¤æ›\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ç’°å¢ƒè¨­å®š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶\n",
    "!pip install transformers torch accelerate bitsandbytes\n",
    "!pip install google-colab-utils\n",
    "print(\"âœ… å¥—ä»¶å®‰è£å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from google.colab import drive, files\n",
    "\n",
    "print(f\"ğŸš€ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"ğŸ”¥ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ’» GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Google Drive é€£æ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›è¼‰ Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# è¨­å®šå·¥ä½œç›®éŒ„\n",
    "DRIVE_BASE = '/content/drive/MyDrive/VACA_LLM'\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_BASE}/requests', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_BASE}/responses', exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ å·¥ä½œç›®éŒ„ï¼š{DRIVE_BASE}\")\n",
    "print(\"âœ… Google Drive é€£æ¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  æ¨¡å‹è¼‰å…¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# é…ç½® 4-bit é‡åŒ–ä»¥ç¯€çœè¨˜æ†¶é«”\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"ğŸ”„ æ­£åœ¨è¼‰å…¥ Qwen2.5-7B-Instruct æ¨¡å‹...\")\nprint(\"â³ é è¨ˆéœ€è¦ 3-5 åˆ†é˜ï¼Œè«‹è€å¿ƒç­‰å€™...\")\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# è¼‰å…¥ tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# ğŸ”§ å®Œæ•´ä¿®å¾© tokenizer è¨­å®šå•é¡Œ\nif tokenizer.pad_token is None:\n    # ä½¿ç”¨ä¸åŒçš„ token ä½œç‚º pad_tokenï¼Œé¿å…å’Œ eos_token è¡çª\n    if tokenizer.unk_token is not None:\n        tokenizer.pad_token = tokenizer.unk_token\n        print(f\"âœ… è¨­å®š pad_token ç‚º unk_token: {tokenizer.unk_token}\")\n    else:\n        # å¦‚æœæ²’æœ‰ unk_tokenï¼Œæ·»åŠ æ–°çš„ç‰¹æ®Š token\n        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n        print(\"âœ… æ·»åŠ æ–°çš„ pad_token: <pad>\")\n\n# è¨­å®š attention mask ç›¸é—œåƒæ•¸\ntokenizer.padding_side = \"left\"  # æ”¹ç‚ºå·¦å´ padding\nprint(\"âœ… è¨­å®š padding_side ç‚º left\")\n\n# ç¢ºä¿ tokenizer è¨­å®šä¸€è‡´æ€§\nprint(f\"ğŸ“Š tokenizer è³‡è¨Š:\")\nprint(f\"   - pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\nprint(f\"   - eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\nprint(f\"   - unk_token: {tokenizer.unk_token} (id: {tokenizer.unk_token_id if tokenizer.unk_token else 'None'})\")\nprint(f\"   - vocab_size: {len(tokenizer)}\")\n\n# ğŸ”§ è¼‰å…¥æ¨¡å‹ï¼ˆç§»é™¤ flash attention ä¾è³´ï¼‰\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        # ç§»é™¤ flash_attention_2 ä»¥é¿å…ä¾è³´å•é¡Œ\n    )\n    print(\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆä½¿ç”¨æ¨™æº– attentionï¼‰\")\nexcept Exception as e:\n    print(f\"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}\")\n    print(\"ğŸ”„ å˜—è©¦ä½¿ç”¨å‚™ç”¨é…ç½®...\")\n    \n    # å‚™ç”¨è¼‰å…¥æ–¹å¼\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    )\n    print(\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ˆå‚™ç”¨é…ç½®ï¼‰\")\n\n# ğŸ”§ å¦‚æœæ·»åŠ äº†æ–°çš„ tokenï¼Œéœ€è¦èª¿æ•´æ¨¡å‹ embeddings\nif tokenizer.pad_token == \"<pad>\":\n    model.resize_token_embeddings(len(tokenizer))\n    print(\"âœ… èª¿æ•´æ¨¡å‹ token embeddings å¤§å°\")\n\nprint(\"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼\")\nprint(f\"ğŸ“Š æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Prompt Template è¨­è¨ˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_vocabulary_prompt(request_data):\n    \"\"\"\n    æ ¹æ“šè«‹æ±‚è³‡æ–™å»ºç«‹å–®å­—ç”Ÿæˆçš„ prompt - ä¿®å¾©ç‰ˆï¼ˆç”Ÿæˆè‹±æ–‡ï¼‰\n    \n    request_data æ ¼å¼:\n    {\n        \"count\": 5,\n        \"tags\": [\"IELTS\", \"academic\"],\n        \"difficulty\": \"6.5-7.5\", \n        \"learned_words_summary\": \"å·²å­¸ç¿’2000å€‹å¸¸ç”¨è©ï¼ŒåŒ…å«åŸºç¤å‹•è©ã€å½¢å®¹è©\",\n        \"avoid_words\": [\"abandon\", \"ability\"] // æœ€è¿‘å·²èƒŒçš„è©\n    }\n    \"\"\"\n    count = request_data.get('count', 5)\n    tags = request_data.get('tags', ['general'])\n    difficulty = request_data.get('difficulty', 'intermediate')\n    learned_summary = request_data.get('learned_words_summary', 'beginner level')\n    avoid_words = request_data.get('avoid_words', [])\n    \n    avoid_text = \"\"\n    if avoid_words:\n        avoid_text = f\"\\n- Avoid these recently studied words: {', '.join(avoid_words[:20])}\"\n    \n    tags_text = ', '.join(tags)\n    \n    # ğŸŒ ä¿®æ”¹ï¼šç”Ÿæˆè‹±æ–‡ JSONï¼Œé¿å…ç·¨ç¢¼å•é¡Œ\n    prompt = f\"\"\"You are a professional English vocabulary card generator. Please generate {count} high-quality English vocabulary cards according to the following requirements.\n\n## User Requirements:\n- Tag categories: {tags_text}\n- Difficulty level: {difficulty}\n- Learning background: {learned_summary}{avoid_text}\n\n## Generation Rules:\n1. Choose words suitable for the specified difficulty level\n2. Avoid repeating already learned words\n3. Provide accurate English definitions and meanings\n4. Include practical example sentences\n5. Mark parts of speech\n\n## Output Format (Strict JSON, ALL ENGLISH):\n```json\n{{\n  \"cards\": [\n    {{\n      \"word\": {{\n        \"base\": \"example\",\n        \"phonetic\": \"/ÉªÉ¡ËˆzÃ¦mpÉ™l/\",\n        \"forms\": [\n          {{\"pos\": \"n.\", \"form\": \"examples\"}}\n        ]\n      }},\n      \"posPrimary\": \"n.\",\n      \"meaning\": \"a thing characteristic of its kind or illustrating a general rule\",\n      \"definition\": \"an instance serving for illustration\",\n      \"synonyms\": [\"instance\", \"case\", \"illustration\"],\n      \"antonyms\": [\"exception\"],\n      \"example\": \"This is a good example of effective communication.\",\n      \"tags\": [\"{tags_text.split(', ')[0] if tags else 'general'}\"],\n      \"anchors\": [],\n      \"translation_key\": \"example\"\n    }}\n  ]\n}}\n```\n\nIMPORTANT: \n- ALL text content MUST be in English only\n- No Chinese characters in the JSON output\n- Use 'translation_key' field for frontend translation lookup\n- Meaning and definition should be clear English explanations\n\nPlease generate {count} vocabulary cards that meet the requirements:\"\"\"\n    \n    return prompt\n\n# æ¸¬è©¦ prompt ç”Ÿæˆ\ntest_request = {\n    \"count\": 3,\n    \"tags\": [\"IELTS\", \"academic\"], \n    \"difficulty\": \"6.5-7.5\",\n    \"learned_words_summary\": \"Have learned 2000 basic words\",\n    \"avoid_words\": [\"abandon\", \"ability\", \"abstract\"]\n}\n\nprint(\"ğŸ“ æ¸¬è©¦ Prompt (è‹±æ–‡ç‰ˆ):\")\nprint(create_vocabulary_prompt(test_request)[:500] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å–®å­—ç”Ÿæˆæ ¸å¿ƒå‡½æ•¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_vocabulary_cards(request_data, max_retries=3):\n    \"\"\"\n    ç”Ÿæˆå–®å­—å¡ç‰‡çš„æ ¸å¿ƒå‡½æ•¸ - çµ‚æ¥µä¿®å¾©ç‰ˆ\n    \"\"\"\n    prompt = create_vocabulary_prompt(request_data)\n    \n    for attempt in range(max_retries):\n        try:\n            print(f\"ğŸ”„ ç”Ÿæˆå˜—è©¦ {attempt + 1}/{max_retries}...\")\n            \n            # ğŸ”§ æ”¹é€²çš„ tokenizer ç·¨ç¢¼ - é¿å… tuple index å•é¡Œ\n            try:\n                inputs = tokenizer(\n                    prompt, \n                    return_tensors=\"pt\", \n                    truncation=True,\n                    max_length=1800,  # é€²ä¸€æ­¥æ¸›å°‘è¼¸å…¥é•·åº¦\n                    padding=False,    # å–®å€‹è¼¸å…¥ä¸éœ€è¦ padding\n                    add_special_tokens=True\n                )\n                \n                # æ‰‹å‹•ç§»è‡³è¨­å‚™\n                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n                \n                print(f\"ğŸ“Š è¼¸å…¥ tokens æ•¸é‡: {inputs['input_ids'].shape[-1]}\")\n                \n            except Exception as tokenizer_error:\n                print(f\"âŒ Tokenizer éŒ¯èª¤: {tokenizer_error}\")\n                continue\n            \n            # ğŸš€ å¤§å¹…å„ªåŒ–çš„ç”Ÿæˆåƒæ•¸ - è¿½æ±‚é€Ÿåº¦å’Œç©©å®šæ€§\n            generation_config = {\n                \"max_new_tokens\": 500,     # æ¸›å°‘è‡³500 tokens\n                \"min_new_tokens\": 80,      # ç¢ºä¿æœ€å°é•·åº¦\n                \"temperature\": 0.2,        # æ›´ä½æº«åº¦æé«˜ä¸€è‡´æ€§\n                \"top_p\": 0.9,             # ç¨é«˜çš„top_pä¿æŒå¤šæ¨£æ€§\n                \"do_sample\": True,\n                \"pad_token_id\": tokenizer.pad_token_id,\n                \"eos_token_id\": tokenizer.eos_token_id,\n                \"repetition_penalty\": 1.05,\n                \"length_penalty\": 0.9,     # é¼“å‹µè¼ƒçŸ­å›æ‡‰\n                \"early_stopping\": True,\n                \"use_cache\": True,         # å•Ÿç”¨ KV cache\n                \"num_beams\": 1,           # é—œé–‰ beam search åŠ é€Ÿ\n            }\n            \n            # é–‹å§‹ç”Ÿæˆ - åŠ ä¸Šæ›´å¤šéŒ¯èª¤è™•ç†\n            start_time = time.time()\n            \n            try:\n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        **generation_config\n                    )\n                \n                generation_time = time.time() - start_time\n                print(f\"â±ï¸  ç”Ÿæˆæ™‚é–“: {generation_time:.2f}ç§’\")\n                \n            except Exception as generation_error:\n                print(f\"âŒ ç”ŸæˆéŒ¯èª¤: {generation_error}\")\n                continue\n            \n            # ğŸ”§ å®‰å…¨çš„è§£ç¢¼å›æ‡‰ - ä¿®å¾© tuple index å•é¡Œ\n            try:\n                # ç¢ºä¿ outputs æ˜¯æ­£ç¢ºæ ¼å¼\n                if isinstance(outputs, tuple):\n                    outputs = outputs[0]  # å–ç¬¬ä¸€å€‹å…ƒç´ \n                elif hasattr(outputs, 'sequences'):\n                    outputs = outputs.sequences\n                \n                # æª¢æŸ¥ outputs å½¢ç‹€\n                if len(outputs.shape) != 2:\n                    print(f\"âŒ ç•°å¸¸çš„ outputs å½¢ç‹€: {outputs.shape}\")\n                    continue\n                \n                # å®‰å…¨åœ°è¨ˆç®— input é•·åº¦\n                input_length = inputs['input_ids'].shape[-1]\n                if outputs.shape[-1] <= input_length:\n                    print(f\"âŒ ç”Ÿæˆé•·åº¦ç•°å¸¸: {outputs.shape[-1]} <= {input_length}\")\n                    continue\n                \n                # åªè§£ç¢¼æ–°ç”Ÿæˆçš„éƒ¨åˆ†\n                new_tokens = outputs[0][input_length:]\n                response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n                \n                print(f\"ğŸ“Š æ–°ç”Ÿæˆ tokens: {len(new_tokens)}\")\n                print(f\"ğŸ“„ å›æ‡‰é è¦½: {response[:150]}...\")\n                \n            except Exception as decode_error:\n                print(f\"âŒ è§£ç¢¼éŒ¯èª¤: {decode_error}\")\n                import traceback\n                traceback.print_exc()\n                continue\n            \n            # ğŸ¯ å¼·åŒ–çš„ JSON è§£æç­–ç•¥\n            result = None\n            \n            # ç­–ç•¥ 1: ç›´æ¥å°‹æ‰¾ JSON å°è±¡ (æœ€å¸¸è¦‹æƒ…æ³)\n            try:\n                # ç§»é™¤æ‰€æœ‰éJSONå­—ç¬¦\n                import re\n                # æ‰¾åˆ°ç¬¬ä¸€å€‹å®Œæ•´çš„ JSON å°è±¡\n                json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n                matches = re.findall(json_pattern, response, re.DOTALL)\n                \n                for match in matches:\n                    try:\n                        test_result = json.loads(match)\n                        if 'cards' in test_result:\n                            result = test_result\n                            print(\"âœ… ç­–ç•¥1æˆåŠŸ: æ­£è¦è¡¨é”å¼æå– JSON\")\n                            break\n                    except:\n                        continue\n            except:\n                pass\n            \n            # ç­–ç•¥ 2: æ‰¾å°‹ ```json æ¨™è¨˜\n            if not result:\n                try:\n                    json_start = response.find('```json')\n                    if json_start != -1:\n                        json_end = response.find('```', json_start + 7)\n                        if json_end != -1:\n                            json_text = response[json_start + 7:json_end].strip()\n                            result = json.loads(json_text)\n                            print(\"âœ… ç­–ç•¥2æˆåŠŸ: Markdown JSON å€å¡Š\")\n                except:\n                    pass\n            \n            # ç­–ç•¥ 3: é€å­—ç¬¦æœå°‹å®Œæ•´ JSON\n            if not result:\n                try:\n                    start_idx = response.find('{')\n                    if start_idx != -1:\n                        bracket_count = 0\n                        for i, char in enumerate(response[start_idx:]):\n                            if char == '{':\n                                bracket_count += 1\n                            elif char == '}':\n                                bracket_count -= 1\n                                if bracket_count == 0:\n                                    json_text = response[start_idx:start_idx + i + 1]\n                                    try:\n                                        result = json.loads(json_text)\n                                        print(\"âœ… ç­–ç•¥3æˆåŠŸ: æ‹¬è™ŸåŒ¹é…è§£æ\")\n                                        break\n                                    except:\n                                        continue\n                except:\n                    pass\n            \n            # ğŸ“Š é©—è­‰çµæœæ ¼å¼ä¸¦è¿”å›\n            if result and isinstance(result, dict) and 'cards' in result:\n                cards = result['cards']\n                if isinstance(cards, list) and len(cards) > 0:\n                    # é©—è­‰æ¯å€‹å¡ç‰‡çš„å¿…è¦æ¬„ä½\n                    valid_cards = []\n                    for card in cards:\n                        if all(key in card for key in ['word', 'meaning', 'example']):\n                            valid_cards.append(card)\n                    \n                    if valid_cards:\n                        result['cards'] = valid_cards\n                        print(f\"âœ… æˆåŠŸç”Ÿæˆ {len(valid_cards)} å€‹æœ‰æ•ˆå–®å­—å¡\")\n                        return result\n            \n            print(\"âŒ JSON è§£ææˆ–é©—è­‰å¤±æ•—\")\n            print(f\"ğŸ“„ å®Œæ•´å›æ‡‰: {response[:800]}...\")\n                \n        except Exception as e:\n            print(f\"âŒ å˜—è©¦ {attempt + 1} ç™¼ç”ŸéŒ¯èª¤: {e}\")\n            import traceback\n            traceback.print_exc()\n            \n            # å¦‚æœæ˜¯è¨˜æ†¶é«”å•é¡Œï¼Œæ¸…ç†ä¸€ä¸‹\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    \n    print(\"âŒ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œç”Ÿæˆå¤±æ•—\")\n    return None\n\nprint(\"âœ… çµ‚æ¥µä¿®å¾©ç‰ˆå–®å­—ç”Ÿæˆå‡½æ•¸æº–å‚™å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Google Drive æª”æ¡ˆç›£æ§ç³»çµ±\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_request_file(filepath):\n    \"\"\"\n    è™•ç†è«‹æ±‚æª”æ¡ˆ - ä¿®å¾©ç·¨ç¢¼ç‰ˆ\n    \"\"\"\n    try:\n        # ğŸ“– è®€å–è«‹æ±‚ - ç¢ºä¿ UTF-8 ç·¨ç¢¼\n        with open(filepath, 'r', encoding='utf-8') as f:\n            request_data = json.load(f)\n        \n        print(f\"ğŸ“¥ æ”¶åˆ°è«‹æ±‚: {request_data}\")\n        \n        # ç”Ÿæˆå–®å­—å¡\n        result = generate_vocabulary_cards(request_data)\n        \n        if result:\n            # ç”¢ç”Ÿå›æ‡‰æª”æ¡ˆ\n            response_filename = os.path.basename(filepath).replace('request_', 'response_')\n            response_path = f\"{DRIVE_BASE}/responses/{response_filename}\"\n            \n            # åŠ ä¸Šæ™‚é–“æˆ³è¨˜å’Œç‹€æ…‹\n            response_data = {\n                \"status\": \"success\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"request\": request_data,\n                \"result\": result,\n                \"encoding_info\": {\n                    \"format\": \"english_json\",\n                    \"requires_translation\": True,\n                    \"version\": \"1.1.0\"\n                }\n            }\n            \n            # ğŸ”§ å¼·åŒ–çš„ç·¨ç¢¼å¯«å…¥ - ç¢ºä¿ç„¡äº‚ç¢¼\n            try:\n                # æ–¹æ³•1: æ¨™æº– UTF-8 å¯«å…¥\n                with open(response_path, 'w', encoding='utf-8', newline='') as f:\n                    json.dump(response_data, f, ensure_ascii=False, indent=2, separators=(',', ': '))\n                print(f\"âœ… å›æ‡‰å·²ä¿å­˜ï¼ˆUTF-8ï¼‰: {response_path}\")\n                \n                # ğŸ” é©—è­‰å¯«å…¥çµæœ\n                with open(response_path, 'r', encoding='utf-8') as f:\n                    verification = json.load(f)\n                    print(f\"ğŸ“Š é©—è­‰æˆåŠŸ: {len(verification.get('result', {}).get('cards', []))} å¼µå¡ç‰‡\")\n                    \n            except UnicodeEncodeError as encoding_error:\n                print(f\"âš ï¸ UTF-8 ç·¨ç¢¼å¤±æ•—ï¼Œå˜—è©¦å‚™ç”¨æ–¹æ¡ˆ: {encoding_error}\")\n                \n                # æ–¹æ³•2: ASCII å®‰å…¨å¯«å…¥ (å‚™ç”¨)\n                with open(response_path, 'w', encoding='utf-8') as f:\n                    json.dump(response_data, f, ensure_ascii=True, indent=2)\n                print(f\"âœ… å›æ‡‰å·²ä¿å­˜ï¼ˆASCII å®‰å…¨ï¼‰: {response_path}\")\n                \n        else:\n            # ç”Ÿæˆå¤±æ•—çš„å›æ‡‰\n            response_filename = os.path.basename(filepath).replace('request_', 'error_')\n            response_path = f\"{DRIVE_BASE}/responses/{response_filename}\"\n            \n            error_data = {\n                \"status\": \"error\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"request\": request_data,\n                \"error\": \"Failed to generate vocabulary cards\",\n                \"error_code\": \"GENERATION_FAILED\"\n            }\n            \n            with open(response_path, 'w', encoding='utf-8') as f:\n                json.dump(error_data, f, ensure_ascii=False, indent=2)\n                \n            print(f\"âŒ éŒ¯èª¤å›æ‡‰å·²ä¿å­˜: {response_path}\")\n        \n        # åˆªé™¤å·²è™•ç†çš„è«‹æ±‚æª”æ¡ˆ\n        os.remove(filepath)\n        print(f\"ğŸ—‘ï¸  å·²åˆªé™¤è«‹æ±‚æª”æ¡ˆ: {filepath}\")\n        \n    except Exception as e:\n        print(f\"âŒ è™•ç†è«‹æ±‚æª”æ¡ˆéŒ¯èª¤: {e}\")\n        import traceback\n        traceback.print_exc()\n\ndef monitor_requests(check_interval=10):\n    \"\"\"\n    ç›£æ§è«‹æ±‚æª”æ¡ˆçš„ä¸»å¾ªç’° - å¼·åŒ–ç‰ˆ\n    \"\"\"\n    print(f\"ğŸ” é–‹å§‹ç›£æ§è«‹æ±‚æª”æ¡ˆ (æ¯ {check_interval} ç§’æª¢æŸ¥ä¸€æ¬¡)\")\n    print(f\"ğŸ“ ç›£æ§ç›®éŒ„: {DRIVE_BASE}/requests/\")\n    print(\"ğŸŒ ç”Ÿæˆæ ¼å¼: è‹±æ–‡ JSON (é¿å…ç·¨ç¢¼å•é¡Œ)\")\n    print(\"ğŸ’¡ æç¤ºï¼šæŒ‰ Ctrl+C åœæ­¢ç›£æ§\")\n    \n    try:\n        while True:\n            requests_dir = f\"{DRIVE_BASE}/requests\"\n            \n            # æª¢æŸ¥æ˜¯å¦æœ‰æ–°çš„è«‹æ±‚æª”æ¡ˆ\n            if os.path.exists(requests_dir):\n                request_files = [f for f in os.listdir(requests_dir) \n                               if f.startswith('request_') and f.endswith('.json')]\n                \n                for filename in request_files:\n                    filepath = os.path.join(requests_dir, filename)\n                    print(f\"\\nğŸ¯ ç™¼ç¾æ–°è«‹æ±‚: {filename}\")\n                    process_request_file(filepath)\n                \n                if not request_files:\n                    print(\".\", end=\"\", flush=True)  # é¡¯ç¤ºæ´»å‹•æŒ‡ç¤ºå™¨\n            else:\n                print(f\"âš ï¸ è«‹æ±‚ç›®éŒ„ä¸å­˜åœ¨: {requests_dir}\")\n                time.sleep(5)\n            \n            time.sleep(check_interval)\n            \n    except KeyboardInterrupt:\n        print(\"\\nğŸ›‘ ç›£æ§å·²åœæ­¢\")\n    except Exception as e:\n        print(f\"\\nâŒ ç›£æ§éŒ¯èª¤: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(\"âœ… æª”æ¡ˆç›£æ§ç³»çµ±æº–å‚™å®Œæˆï¼ˆç·¨ç¢¼ä¿®å¾©ç‰ˆï¼‰\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª æ¸¬è©¦ç”ŸæˆåŠŸèƒ½\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ§ª æ”¹é€²çš„æ¸¬è©¦ç³»çµ±\ndef run_comprehensive_test():\n    \"\"\"\n    åŸ·è¡Œå…¨é¢çš„è¨ºæ–·æ¸¬è©¦\n    \"\"\"\n    print(\"ğŸ” === VACA LLM è¨ºæ–·æ¸¬è©¦é–‹å§‹ ===\")\n    \n    # 1. æª¢æŸ¥æ¨¡å‹å’Œ tokenizer ç‹€æ…‹\n    print(\"\\n1ï¸âƒ£ æª¢æŸ¥æ¨¡å‹ç‹€æ…‹:\")\n    if 'model' in globals() and 'tokenizer' in globals():\n        print(\"âœ… æ¨¡å‹å’Œ tokenizer å·²è¼‰å…¥\")\n        print(f\"   - æ¨¡å‹é¡å‹: {type(model).__name__}\")\n        print(f\"   - Tokenizer é¡å‹: {type(tokenizer).__name__}\")\n        print(f\"   - pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n        print(f\"   - eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n        \n        if torch.cuda.is_available():\n            print(f\"   - GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    else:\n        print(\"âŒ æ¨¡å‹æˆ– tokenizer æœªè¼‰å…¥\")\n        return False\n    \n    # 2. æ¸¬è©¦ tokenizer\n    print(\"\\n2ï¸âƒ£ æ¸¬è©¦ Tokenizer:\")\n    try:\n        test_text = \"Hello, this is a test.\"\n        tokens = tokenizer(test_text, return_tensors=\"pt\", padding=False)\n        print(f\"âœ… Tokenizer æ­£å¸¸å·¥ä½œ\")\n        print(f\"   - æ¸¬è©¦æ–‡æœ¬: {test_text}\")\n        print(f\"   - Token æ•¸é‡: {tokens['input_ids'].shape[-1]}\")\n    except Exception as e:\n        print(f\"âŒ Tokenizer éŒ¯èª¤: {e}\")\n        return False\n    \n    # 3. æ¸¬è©¦ç°¡å–®ç”Ÿæˆ\n    print(\"\\n3ï¸âƒ£ æ¸¬è©¦ç°¡å–®ç”Ÿæˆ:\")\n    try:\n        simple_prompt = \"Generate a simple English word: \"\n        inputs = tokenizer(simple_prompt, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.7,\n                pad_token_id=tokenizer.pad_token_id,\n                do_sample=True\n            )\n        \n        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n        print(f\"âœ… ç°¡å–®ç”Ÿæˆæ¸¬è©¦æˆåŠŸ\")\n        print(f\"   - è¼¸å…¥: {simple_prompt}\")\n        print(f\"   - è¼¸å‡º: {response}\")\n        \n    except Exception as e:\n        print(f\"âŒ ç°¡å–®ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}\")\n        return False\n    \n    # 4. æ¸¬è©¦å–®å­—å¡ç”Ÿæˆ\n    print(\"\\n4ï¸âƒ£ æ¸¬è©¦å–®å­—å¡ç”Ÿæˆ:\")\n    \n    # ä½¿ç”¨æœ€ç°¡å–®çš„è«‹æ±‚\n    minimal_request = {\n        \"count\": 1,\n        \"tags\": [\"basic\"],\n        \"difficulty\": \"easy\", \n        \"learned_words_summary\": \"beginner level\",\n        \"avoid_words\": []\n    }\n    \n    print(f\"ğŸ“ æ¸¬è©¦è«‹æ±‚: {json.dumps(minimal_request, indent=2)}\")\n    \n    try:\n        result = generate_vocabulary_cards(minimal_request, max_retries=2)\n        \n        if result:\n            print(\"ğŸ‰ å–®å­—å¡ç”Ÿæˆæ¸¬è©¦æˆåŠŸï¼\")\n            print(\"ğŸ“„ ç”Ÿæˆçµæœ:\")\n            print(json.dumps(result, ensure_ascii=False, indent=2))\n            \n            # è©³ç´°é©—è­‰\n            cards = result.get('cards', [])\n            if cards:\n                card = cards[0]\n                print(f\"\\nğŸ” å¡ç‰‡é©—è­‰:\")\n                for field in ['word', 'meaning', 'example']:\n                    if field in card:\n                        print(f\"   âœ… {field}: {card[field]}\")\n                    else:\n                        print(f\"   âŒ ç¼ºå°‘ {field}\")\n                        \n                print(\"âœ… å…¨éƒ¨æ¸¬è©¦é€šéï¼ç³»çµ±å·²æº–å‚™å°±ç·’ã€‚\")\n                return True\n            else:\n                print(\"âŒ ç”Ÿæˆçš„å¡ç‰‡é™£åˆ—ç‚ºç©º\")\n                return False\n        else:\n            print(\"âŒ å–®å­—å¡ç”Ÿæˆå¤±æ•—\")\n            return False\n            \n    except Exception as e:\n        print(f\"âŒ å–®å­—å¡ç”Ÿæˆæ¸¬è©¦éŒ¯èª¤: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# åŸ·è¡Œå®Œæ•´æ¸¬è©¦\nprint(\"ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´è¨ºæ–·...\")\nsuccess = run_comprehensive_test()\n\nif success:\n    print(\"\\nğŸ¯ === æ¸¬è©¦çµæœï¼šç³»çµ±æ­£å¸¸ï¼===\")\n    print(\"âœ… å¯ä»¥é–‹å§‹ç›£æ§ Google Drive æª”æ¡ˆ\")\n    print(\"âœ… æº–å‚™è™•ç†å‰ç«¯è«‹æ±‚\")\nelse:\n    print(\"\\nâš ï¸ === æ¸¬è©¦çµæœï¼šç™¼ç¾å•é¡Œ ===\")\n    print(\"ğŸ”§ è«‹æª¢æŸ¥ä¸Šé¢çš„éŒ¯èª¤è¨Šæ¯ä¸¦é‡æ–°åŸ·è¡Œç›¸é—œè¨­å®š\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ å•Ÿå‹•ç›£æ§æœå‹™\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ VACA LLM å–®å­—ç”Ÿæˆå™¨å·²æº–å‚™å°±ç·’ï¼\")\n",
    "print()\n",
    "print(\"ğŸ“‹ ä½¿ç”¨èªªæ˜ï¼š\")\n",
    "print(f\"1. å‰ç«¯æ‡‰ç”¨å°‡è«‹æ±‚å¯«å…¥: {DRIVE_BASE}/requests/request_[timestamp].json\")\n",
    "print(f\"2. æ­¤æœå‹™å°‡å›æ‡‰å¯«å…¥: {DRIVE_BASE}/responses/response_[timestamp].json\")\n",
    "print(\"3. è«‹æ±‚æ ¼å¼åƒè€ƒä¸Šæ–¹çš„ test_request\")\n",
    "print()\n",
    "print(\"ğŸ”§ è«‹æ±‚æª”æ¡ˆæ ¼å¼:\")\n",
    "print(json.dumps(test_request, ensure_ascii=False, indent=2))\n",
    "print()\n",
    "\n",
    "# å•Ÿå‹•ç›£æ§\n",
    "monitor_requests(check_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ æ‰‹å‹•æ¸¬è©¦å€åŸŸ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹å‹•å»ºç«‹æ¸¬è©¦è«‹æ±‚æª”æ¡ˆ\n",
    "manual_request = {\n",
    "    \"count\": 3,\n",
    "    \"tags\": [\"TOEFL\", \"academic\"],\n",
    "    \"difficulty\": \"7.0-8.0\",\n",
    "    \"learned_words_summary\": \"å·²æŒæ¡åŸºç¤è©å½™ï¼Œæ­£åœ¨æº–å‚™æ‰˜ç¦è€ƒè©¦\",\n",
    "    \"avoid_words\": [\"achieve\", \"analysis\", \"approach\", \"area\", \"assessment\"]\n",
    "}\n",
    "\n",
    "# å¯«å…¥è«‹æ±‚æª”æ¡ˆ\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "request_filename = f\"request_{timestamp}.json\"\n",
    "request_path = f\"{DRIVE_BASE}/requests/{request_filename}\"\n",
    "\n",
    "with open(request_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(manual_request, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… å·²å»ºç«‹æ¸¬è©¦è«‹æ±‚æª”æ¡ˆ: {request_path}\")\n",
    "print(\"ğŸ” ç›£æ§ç³»çµ±å°‡æœƒè‡ªå‹•è™•ç†æ­¤è«‹æ±‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ç³»çµ±ç‹€æ…‹æª¢æŸ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_status():\n",
    "    print(\"ğŸ” ç³»çµ±ç‹€æ…‹æª¢æŸ¥:\")\n",
    "    print(f\"ğŸ“ å·¥ä½œç›®éŒ„: {DRIVE_BASE}\")\n",
    "    \n",
    "    # æª¢æŸ¥ç›®éŒ„\n",
    "    requests_dir = f\"{DRIVE_BASE}/requests\"\n",
    "    responses_dir = f\"{DRIVE_BASE}/responses\"\n",
    "    \n",
    "    print(f\"ğŸ“¥ è«‹æ±‚ç›®éŒ„: {requests_dir}\")\n",
    "    if os.path.exists(requests_dir):\n",
    "        request_files = os.listdir(requests_dir)\n",
    "        print(f\"   å¾…è™•ç†è«‹æ±‚: {len(request_files)} å€‹\")\n",
    "        for f in request_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹\n",
    "            print(f\"   - {f}\")\n",
    "    \n",
    "    print(f\"ğŸ“¤ å›æ‡‰ç›®éŒ„: {responses_dir}\")\n",
    "    if os.path.exists(responses_dir):\n",
    "        response_files = os.listdir(responses_dir)\n",
    "        print(f\"   å·²å®Œæˆå›æ‡‰: {len(response_files)} å€‹\")\n",
    "        for f in sorted(response_files)[-3:]:  # é¡¯ç¤ºæœ€è¿‘3å€‹\n",
    "            print(f\"   - {f}\")\n",
    "    \n",
    "    # è¨˜æ†¶é«”ä½¿ç”¨\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"ğŸ’¾ GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.2f} GB / {memory_cached:.2f} GB (allocated/cached)\")\n",
    "    \n",
    "    print(\"âœ… ç³»çµ±ç‹€æ…‹æª¢æŸ¥å®Œæˆ\")\n",
    "\n",
    "check_system_status()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}