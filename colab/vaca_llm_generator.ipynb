{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yourusername/vaca-app/blob/main/colab/vaca_llm_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 VACA App - LLM 單字卡生成器\n",
    "\n",
    "使用 Qwen2.5 7B 模型為 VACA 背單字 App 生成高品質單字卡片。\n",
    "\n",
    "## 功能特色：\n",
    "- 🧠 使用 Qwen2.5 7B Instruct 模型\n",
    "- 📚 支援考試標籤（IELTS、TOEFL、GRE 等）\n",
    "- 🎯 避免已背單字重複\n",
    "- 📄 標準 JSON 格式輸出\n",
    "- 💾 Google Drive 檔案交換\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 環境設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝必要套件\n",
    "!pip install transformers torch accelerate bitsandbytes\n",
    "!pip install google-colab-utils\n",
    "print(\"✅ 套件安裝完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from google.colab import drive, files\n",
    "\n",
    "print(f\"🚀 PyTorch Version: {torch.__version__}\")\n",
    "print(f\"🔥 CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"💻 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔗 Google Drive 連接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 掛載 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 設定工作目錄\n",
    "DRIVE_BASE = '/content/drive/MyDrive/VACA_LLM'\n",
    "os.makedirs(DRIVE_BASE, exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_BASE}/requests', exist_ok=True)\n",
    "os.makedirs(f'{DRIVE_BASE}/responses', exist_ok=True)\n",
    "\n",
    "print(f\"📁 工作目錄：{DRIVE_BASE}\")\n",
    "print(\"✅ Google Drive 連接完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 模型載入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 配置 4-bit 量化以節省記憶體\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\nprint(\"🔄 正在載入 Qwen2.5-7B-Instruct 模型...\")\nprint(\"⏳ 預計需要 3-5 分鐘，請耐心等候...\")\n\nmodel_name = \"Qwen/Qwen2.5-7B-Instruct\"\n\n# 載入 tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 🔧 完整修復 tokenizer 設定問題\nif tokenizer.pad_token is None:\n    # 使用不同的 token 作為 pad_token，避免和 eos_token 衝突\n    if tokenizer.unk_token is not None:\n        tokenizer.pad_token = tokenizer.unk_token\n        print(f\"✅ 設定 pad_token 為 unk_token: {tokenizer.unk_token}\")\n    else:\n        # 如果沒有 unk_token，添加新的特殊 token\n        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n        print(\"✅ 添加新的 pad_token: <pad>\")\n\n# 設定 attention mask 相關參數\ntokenizer.padding_side = \"left\"  # 改為左側 padding\nprint(\"✅ 設定 padding_side 為 left\")\n\n# 確保 tokenizer 設定一致性\nprint(f\"📊 tokenizer 資訊:\")\nprint(f\"   - pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\nprint(f\"   - eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\nprint(f\"   - unk_token: {tokenizer.unk_token} (id: {tokenizer.unk_token_id if tokenizer.unk_token else 'None'})\")\nprint(f\"   - vocab_size: {len(tokenizer)}\")\n\n# 🔧 載入模型（移除 flash attention 依賴）\ntry:\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        # 移除 flash_attention_2 以避免依賴問題\n    )\n    print(\"✅ 模型載入成功（使用標準 attention）\")\nexcept Exception as e:\n    print(f\"❌ 模型載入失敗: {e}\")\n    print(\"🔄 嘗試使用備用配置...\")\n    \n    # 備用載入方式\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=quantization_config,\n        device_map=\"auto\",\n        trust_remote_code=True,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n    )\n    print(\"✅ 模型載入成功（備用配置）\")\n\n# 🔧 如果添加了新的 token，需要調整模型 embeddings\nif tokenizer.pad_token == \"<pad>\":\n    model.resize_token_embeddings(len(tokenizer))\n    print(\"✅ 調整模型 token embeddings 大小\")\n\nprint(\"✅ 模型載入成功！\")\nprint(f\"📊 模型記憶體使用: {model.get_memory_footprint() / 1024**3:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Prompt Template 設計\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_vocabulary_prompt(request_data):\n    \"\"\"\n    根據請求資料建立單字生成的 prompt - 修復版（生成英文）\n    \n    request_data 格式:\n    {\n        \"count\": 5,\n        \"tags\": [\"IELTS\", \"academic\"],\n        \"difficulty\": \"6.5-7.5\", \n        \"learned_words_summary\": \"已學習2000個常用詞，包含基礎動詞、形容詞\",\n        \"avoid_words\": [\"abandon\", \"ability\"] // 最近已背的詞\n    }\n    \"\"\"\n    count = request_data.get('count', 5)\n    tags = request_data.get('tags', ['general'])\n    difficulty = request_data.get('difficulty', 'intermediate')\n    learned_summary = request_data.get('learned_words_summary', 'beginner level')\n    avoid_words = request_data.get('avoid_words', [])\n    \n    avoid_text = \"\"\n    if avoid_words:\n        avoid_text = f\"\\n- Avoid these recently studied words: {', '.join(avoid_words[:20])}\"\n    \n    tags_text = ', '.join(tags)\n    \n    # 🌍 修改：生成英文 JSON，避免編碼問題\n    prompt = f\"\"\"You are a professional English vocabulary card generator. Please generate {count} high-quality English vocabulary cards according to the following requirements.\n\n## User Requirements:\n- Tag categories: {tags_text}\n- Difficulty level: {difficulty}\n- Learning background: {learned_summary}{avoid_text}\n\n## Generation Rules:\n1. Choose words suitable for the specified difficulty level\n2. Avoid repeating already learned words\n3. Provide accurate English definitions and meanings\n4. Include practical example sentences\n5. Mark parts of speech\n\n## Output Format (Strict JSON, ALL ENGLISH):\n```json\n{{\n  \"cards\": [\n    {{\n      \"word\": {{\n        \"base\": \"example\",\n        \"phonetic\": \"/ɪɡˈzæmpəl/\",\n        \"forms\": [\n          {{\"pos\": \"n.\", \"form\": \"examples\"}}\n        ]\n      }},\n      \"posPrimary\": \"n.\",\n      \"meaning\": \"a thing characteristic of its kind or illustrating a general rule\",\n      \"definition\": \"an instance serving for illustration\",\n      \"synonyms\": [\"instance\", \"case\", \"illustration\"],\n      \"antonyms\": [\"exception\"],\n      \"example\": \"This is a good example of effective communication.\",\n      \"tags\": [\"{tags_text.split(', ')[0] if tags else 'general'}\"],\n      \"anchors\": [],\n      \"translation_key\": \"example\"\n    }}\n  ]\n}}\n```\n\nIMPORTANT: \n- ALL text content MUST be in English only\n- No Chinese characters in the JSON output\n- Use 'translation_key' field for frontend translation lookup\n- Meaning and definition should be clear English explanations\n\nPlease generate {count} vocabulary cards that meet the requirements:\"\"\"\n    \n    return prompt\n\n# 測試 prompt 生成\ntest_request = {\n    \"count\": 3,\n    \"tags\": [\"IELTS\", \"academic\"], \n    \"difficulty\": \"6.5-7.5\",\n    \"learned_words_summary\": \"Have learned 2000 basic words\",\n    \"avoid_words\": [\"abandon\", \"ability\", \"abstract\"]\n}\n\nprint(\"📝 測試 Prompt (英文版):\")\nprint(create_vocabulary_prompt(test_request)[:500] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 單字生成核心函數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_vocabulary_cards(request_data, max_retries=3):\n    \"\"\"\n    生成單字卡片的核心函數 - 終極修復版\n    \"\"\"\n    prompt = create_vocabulary_prompt(request_data)\n    \n    for attempt in range(max_retries):\n        try:\n            print(f\"🔄 生成嘗試 {attempt + 1}/{max_retries}...\")\n            \n            # 🔧 改進的 tokenizer 編碼 - 避免 tuple index 問題\n            try:\n                inputs = tokenizer(\n                    prompt, \n                    return_tensors=\"pt\", \n                    truncation=True,\n                    max_length=1800,  # 進一步減少輸入長度\n                    padding=False,    # 單個輸入不需要 padding\n                    add_special_tokens=True\n                )\n                \n                # 手動移至設備\n                inputs = {k: v.to(model.device) for k, v in inputs.items()}\n                \n                print(f\"📊 輸入 tokens 數量: {inputs['input_ids'].shape[-1]}\")\n                \n            except Exception as tokenizer_error:\n                print(f\"❌ Tokenizer 錯誤: {tokenizer_error}\")\n                continue\n            \n            # 🚀 大幅優化的生成參數 - 追求速度和穩定性\n            generation_config = {\n                \"max_new_tokens\": 500,     # 減少至500 tokens\n                \"min_new_tokens\": 80,      # 確保最小長度\n                \"temperature\": 0.2,        # 更低溫度提高一致性\n                \"top_p\": 0.9,             # 稍高的top_p保持多樣性\n                \"do_sample\": True,\n                \"pad_token_id\": tokenizer.pad_token_id,\n                \"eos_token_id\": tokenizer.eos_token_id,\n                \"repetition_penalty\": 1.05,\n                \"length_penalty\": 0.9,     # 鼓勵較短回應\n                \"early_stopping\": True,\n                \"use_cache\": True,         # 啟用 KV cache\n                \"num_beams\": 1,           # 關閉 beam search 加速\n            }\n            \n            # 開始生成 - 加上更多錯誤處理\n            start_time = time.time()\n            \n            try:\n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        **generation_config\n                    )\n                \n                generation_time = time.time() - start_time\n                print(f\"⏱️  生成時間: {generation_time:.2f}秒\")\n                \n            except Exception as generation_error:\n                print(f\"❌ 生成錯誤: {generation_error}\")\n                continue\n            \n            # 🔧 安全的解碼回應 - 修復 tuple index 問題\n            try:\n                # 確保 outputs 是正確格式\n                if isinstance(outputs, tuple):\n                    outputs = outputs[0]  # 取第一個元素\n                elif hasattr(outputs, 'sequences'):\n                    outputs = outputs.sequences\n                \n                # 檢查 outputs 形狀\n                if len(outputs.shape) != 2:\n                    print(f\"❌ 異常的 outputs 形狀: {outputs.shape}\")\n                    continue\n                \n                # 安全地計算 input 長度\n                input_length = inputs['input_ids'].shape[-1]\n                if outputs.shape[-1] <= input_length:\n                    print(f\"❌ 生成長度異常: {outputs.shape[-1]} <= {input_length}\")\n                    continue\n                \n                # 只解碼新生成的部分\n                new_tokens = outputs[0][input_length:]\n                response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n                \n                print(f\"📊 新生成 tokens: {len(new_tokens)}\")\n                print(f\"📄 回應預覽: {response[:150]}...\")\n                \n            except Exception as decode_error:\n                print(f\"❌ 解碼錯誤: {decode_error}\")\n                import traceback\n                traceback.print_exc()\n                continue\n            \n            # 🎯 強化的 JSON 解析策略\n            result = None\n            \n            # 策略 1: 直接尋找 JSON 對象 (最常見情況)\n            try:\n                # 移除所有非JSON字符\n                import re\n                # 找到第一個完整的 JSON 對象\n                json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n                matches = re.findall(json_pattern, response, re.DOTALL)\n                \n                for match in matches:\n                    try:\n                        test_result = json.loads(match)\n                        if 'cards' in test_result:\n                            result = test_result\n                            print(\"✅ 策略1成功: 正規表達式提取 JSON\")\n                            break\n                    except:\n                        continue\n            except:\n                pass\n            \n            # 策略 2: 找尋 ```json 標記\n            if not result:\n                try:\n                    json_start = response.find('```json')\n                    if json_start != -1:\n                        json_end = response.find('```', json_start + 7)\n                        if json_end != -1:\n                            json_text = response[json_start + 7:json_end].strip()\n                            result = json.loads(json_text)\n                            print(\"✅ 策略2成功: Markdown JSON 區塊\")\n                except:\n                    pass\n            \n            # 策略 3: 逐字符搜尋完整 JSON\n            if not result:\n                try:\n                    start_idx = response.find('{')\n                    if start_idx != -1:\n                        bracket_count = 0\n                        for i, char in enumerate(response[start_idx:]):\n                            if char == '{':\n                                bracket_count += 1\n                            elif char == '}':\n                                bracket_count -= 1\n                                if bracket_count == 0:\n                                    json_text = response[start_idx:start_idx + i + 1]\n                                    try:\n                                        result = json.loads(json_text)\n                                        print(\"✅ 策略3成功: 括號匹配解析\")\n                                        break\n                                    except:\n                                        continue\n                except:\n                    pass\n            \n            # 📊 驗證結果格式並返回\n            if result and isinstance(result, dict) and 'cards' in result:\n                cards = result['cards']\n                if isinstance(cards, list) and len(cards) > 0:\n                    # 驗證每個卡片的必要欄位\n                    valid_cards = []\n                    for card in cards:\n                        if all(key in card for key in ['word', 'meaning', 'example']):\n                            valid_cards.append(card)\n                    \n                    if valid_cards:\n                        result['cards'] = valid_cards\n                        print(f\"✅ 成功生成 {len(valid_cards)} 個有效單字卡\")\n                        return result\n            \n            print(\"❌ JSON 解析或驗證失敗\")\n            print(f\"📄 完整回應: {response[:800]}...\")\n                \n        except Exception as e:\n            print(f\"❌ 嘗試 {attempt + 1} 發生錯誤: {e}\")\n            import traceback\n            traceback.print_exc()\n            \n            # 如果是記憶體問題，清理一下\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n    \n    print(\"❌ 達到最大重試次數，生成失敗\")\n    return None\n\nprint(\"✅ 終極修復版單字生成函數準備完成\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Google Drive 檔案監控系統\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_request_file(filepath):\n    \"\"\"\n    處理請求檔案 - 修復編碼版\n    \"\"\"\n    try:\n        # 📖 讀取請求 - 確保 UTF-8 編碼\n        with open(filepath, 'r', encoding='utf-8') as f:\n            request_data = json.load(f)\n        \n        print(f\"📥 收到請求: {request_data}\")\n        \n        # 生成單字卡\n        result = generate_vocabulary_cards(request_data)\n        \n        if result:\n            # 產生回應檔案\n            response_filename = os.path.basename(filepath).replace('request_', 'response_')\n            response_path = f\"{DRIVE_BASE}/responses/{response_filename}\"\n            \n            # 加上時間戳記和狀態\n            response_data = {\n                \"status\": \"success\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"request\": request_data,\n                \"result\": result,\n                \"encoding_info\": {\n                    \"format\": \"english_json\",\n                    \"requires_translation\": True,\n                    \"version\": \"1.1.0\"\n                }\n            }\n            \n            # 🔧 強化的編碼寫入 - 確保無亂碼\n            try:\n                # 方法1: 標準 UTF-8 寫入\n                with open(response_path, 'w', encoding='utf-8', newline='') as f:\n                    json.dump(response_data, f, ensure_ascii=False, indent=2, separators=(',', ': '))\n                print(f\"✅ 回應已保存（UTF-8）: {response_path}\")\n                \n                # 🔍 驗證寫入結果\n                with open(response_path, 'r', encoding='utf-8') as f:\n                    verification = json.load(f)\n                    print(f\"📊 驗證成功: {len(verification.get('result', {}).get('cards', []))} 張卡片\")\n                    \n            except UnicodeEncodeError as encoding_error:\n                print(f\"⚠️ UTF-8 編碼失敗，嘗試備用方案: {encoding_error}\")\n                \n                # 方法2: ASCII 安全寫入 (備用)\n                with open(response_path, 'w', encoding='utf-8') as f:\n                    json.dump(response_data, f, ensure_ascii=True, indent=2)\n                print(f\"✅ 回應已保存（ASCII 安全）: {response_path}\")\n                \n        else:\n            # 生成失敗的回應\n            response_filename = os.path.basename(filepath).replace('request_', 'error_')\n            response_path = f\"{DRIVE_BASE}/responses/{response_filename}\"\n            \n            error_data = {\n                \"status\": \"error\",\n                \"timestamp\": datetime.now().isoformat(),\n                \"request\": request_data,\n                \"error\": \"Failed to generate vocabulary cards\",\n                \"error_code\": \"GENERATION_FAILED\"\n            }\n            \n            with open(response_path, 'w', encoding='utf-8') as f:\n                json.dump(error_data, f, ensure_ascii=False, indent=2)\n                \n            print(f\"❌ 錯誤回應已保存: {response_path}\")\n        \n        # 刪除已處理的請求檔案\n        os.remove(filepath)\n        print(f\"🗑️  已刪除請求檔案: {filepath}\")\n        \n    except Exception as e:\n        print(f\"❌ 處理請求檔案錯誤: {e}\")\n        import traceback\n        traceback.print_exc()\n\ndef monitor_requests(check_interval=10):\n    \"\"\"\n    監控請求檔案的主循環 - 強化版\n    \"\"\"\n    print(f\"🔍 開始監控請求檔案 (每 {check_interval} 秒檢查一次)\")\n    print(f\"📁 監控目錄: {DRIVE_BASE}/requests/\")\n    print(\"🌍 生成格式: 英文 JSON (避免編碼問題)\")\n    print(\"💡 提示：按 Ctrl+C 停止監控\")\n    \n    try:\n        while True:\n            requests_dir = f\"{DRIVE_BASE}/requests\"\n            \n            # 檢查是否有新的請求檔案\n            if os.path.exists(requests_dir):\n                request_files = [f for f in os.listdir(requests_dir) \n                               if f.startswith('request_') and f.endswith('.json')]\n                \n                for filename in request_files:\n                    filepath = os.path.join(requests_dir, filename)\n                    print(f\"\\n🎯 發現新請求: {filename}\")\n                    process_request_file(filepath)\n                \n                if not request_files:\n                    print(\".\", end=\"\", flush=True)  # 顯示活動指示器\n            else:\n                print(f\"⚠️ 請求目錄不存在: {requests_dir}\")\n                time.sleep(5)\n            \n            time.sleep(check_interval)\n            \n    except KeyboardInterrupt:\n        print(\"\\n🛑 監控已停止\")\n    except Exception as e:\n        print(f\"\\n❌ 監控錯誤: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(\"✅ 檔案監控系統準備完成（編碼修復版）\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 測試生成功能\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 🧪 改進的測試系統\ndef run_comprehensive_test():\n    \"\"\"\n    執行全面的診斷測試\n    \"\"\"\n    print(\"🔍 === VACA LLM 診斷測試開始 ===\")\n    \n    # 1. 檢查模型和 tokenizer 狀態\n    print(\"\\n1️⃣ 檢查模型狀態:\")\n    if 'model' in globals() and 'tokenizer' in globals():\n        print(\"✅ 模型和 tokenizer 已載入\")\n        print(f\"   - 模型類型: {type(model).__name__}\")\n        print(f\"   - Tokenizer 類型: {type(tokenizer).__name__}\")\n        print(f\"   - pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n        print(f\"   - eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n        \n        if torch.cuda.is_available():\n            print(f\"   - GPU 記憶體: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n    else:\n        print(\"❌ 模型或 tokenizer 未載入\")\n        return False\n    \n    # 2. 測試 tokenizer\n    print(\"\\n2️⃣ 測試 Tokenizer:\")\n    try:\n        test_text = \"Hello, this is a test.\"\n        tokens = tokenizer(test_text, return_tensors=\"pt\", padding=False)\n        print(f\"✅ Tokenizer 正常工作\")\n        print(f\"   - 測試文本: {test_text}\")\n        print(f\"   - Token 數量: {tokens['input_ids'].shape[-1]}\")\n    except Exception as e:\n        print(f\"❌ Tokenizer 錯誤: {e}\")\n        return False\n    \n    # 3. 測試簡單生成\n    print(\"\\n3️⃣ 測試簡單生成:\")\n    try:\n        simple_prompt = \"Generate a simple English word: \"\n        inputs = tokenizer(simple_prompt, return_tensors=\"pt\").to(model.device)\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0.7,\n                pad_token_id=tokenizer.pad_token_id,\n                do_sample=True\n            )\n        \n        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[-1]:], skip_special_tokens=True)\n        print(f\"✅ 簡單生成測試成功\")\n        print(f\"   - 輸入: {simple_prompt}\")\n        print(f\"   - 輸出: {response}\")\n        \n    except Exception as e:\n        print(f\"❌ 簡單生成測試失敗: {e}\")\n        return False\n    \n    # 4. 測試單字卡生成\n    print(\"\\n4️⃣ 測試單字卡生成:\")\n    \n    # 使用最簡單的請求\n    minimal_request = {\n        \"count\": 1,\n        \"tags\": [\"basic\"],\n        \"difficulty\": \"easy\", \n        \"learned_words_summary\": \"beginner level\",\n        \"avoid_words\": []\n    }\n    \n    print(f\"📝 測試請求: {json.dumps(minimal_request, indent=2)}\")\n    \n    try:\n        result = generate_vocabulary_cards(minimal_request, max_retries=2)\n        \n        if result:\n            print(\"🎉 單字卡生成測試成功！\")\n            print(\"📄 生成結果:\")\n            print(json.dumps(result, ensure_ascii=False, indent=2))\n            \n            # 詳細驗證\n            cards = result.get('cards', [])\n            if cards:\n                card = cards[0]\n                print(f\"\\n🔍 卡片驗證:\")\n                for field in ['word', 'meaning', 'example']:\n                    if field in card:\n                        print(f\"   ✅ {field}: {card[field]}\")\n                    else:\n                        print(f\"   ❌ 缺少 {field}\")\n                        \n                print(\"✅ 全部測試通過！系統已準備就緒。\")\n                return True\n            else:\n                print(\"❌ 生成的卡片陣列為空\")\n                return False\n        else:\n            print(\"❌ 單字卡生成失敗\")\n            return False\n            \n    except Exception as e:\n        print(f\"❌ 單字卡生成測試錯誤: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# 執行完整測試\nprint(\"🚀 開始執行完整診斷...\")\nsuccess = run_comprehensive_test()\n\nif success:\n    print(\"\\n🎯 === 測試結果：系統正常！===\")\n    print(\"✅ 可以開始監控 Google Drive 檔案\")\n    print(\"✅ 準備處理前端請求\")\nelse:\n    print(\"\\n⚠️ === 測試結果：發現問題 ===\")\n    print(\"🔧 請檢查上面的錯誤訊息並重新執行相關設定\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 啟動監控服務\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 VACA LLM 單字生成器已準備就緒！\")\n",
    "print()\n",
    "print(\"📋 使用說明：\")\n",
    "print(f\"1. 前端應用將請求寫入: {DRIVE_BASE}/requests/request_[timestamp].json\")\n",
    "print(f\"2. 此服務將回應寫入: {DRIVE_BASE}/responses/response_[timestamp].json\")\n",
    "print(\"3. 請求格式參考上方的 test_request\")\n",
    "print()\n",
    "print(\"🔧 請求檔案格式:\")\n",
    "print(json.dumps(test_request, ensure_ascii=False, indent=2))\n",
    "print()\n",
    "\n",
    "# 啟動監控\n",
    "monitor_requests(check_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ 手動測試區域\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手動建立測試請求檔案\n",
    "manual_request = {\n",
    "    \"count\": 3,\n",
    "    \"tags\": [\"TOEFL\", \"academic\"],\n",
    "    \"difficulty\": \"7.0-8.0\",\n",
    "    \"learned_words_summary\": \"已掌握基礎詞彙，正在準備托福考試\",\n",
    "    \"avoid_words\": [\"achieve\", \"analysis\", \"approach\", \"area\", \"assessment\"]\n",
    "}\n",
    "\n",
    "# 寫入請求檔案\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "request_filename = f\"request_{timestamp}.json\"\n",
    "request_path = f\"{DRIVE_BASE}/requests/{request_filename}\"\n",
    "\n",
    "with open(request_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(manual_request, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ 已建立測試請求檔案: {request_path}\")\n",
    "print(\"🔍 監控系統將會自動處理此請求\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 系統狀態檢查\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_system_status():\n",
    "    print(\"🔍 系統狀態檢查:\")\n",
    "    print(f\"📁 工作目錄: {DRIVE_BASE}\")\n",
    "    \n",
    "    # 檢查目錄\n",
    "    requests_dir = f\"{DRIVE_BASE}/requests\"\n",
    "    responses_dir = f\"{DRIVE_BASE}/responses\"\n",
    "    \n",
    "    print(f\"📥 請求目錄: {requests_dir}\")\n",
    "    if os.path.exists(requests_dir):\n",
    "        request_files = os.listdir(requests_dir)\n",
    "        print(f\"   待處理請求: {len(request_files)} 個\")\n",
    "        for f in request_files[:5]:  # 只顯示前5個\n",
    "            print(f\"   - {f}\")\n",
    "    \n",
    "    print(f\"📤 回應目錄: {responses_dir}\")\n",
    "    if os.path.exists(responses_dir):\n",
    "        response_files = os.listdir(responses_dir)\n",
    "        print(f\"   已完成回應: {len(response_files)} 個\")\n",
    "        for f in sorted(response_files)[-3:]:  # 顯示最近3個\n",
    "            print(f\"   - {f}\")\n",
    "    \n",
    "    # 記憶體使用\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"💾 GPU 記憶體使用: {memory_used:.2f} GB / {memory_cached:.2f} GB (allocated/cached)\")\n",
    "    \n",
    "    print(\"✅ 系統狀態檢查完成\")\n",
    "\n",
    "check_system_status()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}